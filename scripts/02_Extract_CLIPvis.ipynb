{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d9d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def dump_data(data, filename):\n",
    "    print('writing file: ' + filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206bf1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d6408b57f8458fa11d68e30fb00227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fmri_stim = np.load(\"/projects/crunchie/boyanova/EEG_Things/eeg_prep/scripts/fmri_train_stim.npy\", allow_pickle=True)\n",
    "main_path_stim = \"/projects/crunchie/boyanova/EEG_Things/data_set/Images\"\n",
    "\n",
    "image_paths = []\n",
    "for im in tqdm(fmri_stim):\n",
    "    im_cat = im.split(\".\")[0][0:-4]\n",
    "    image_paths.append(os.path.join(main_path_stim, im_cat, im))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a11b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4e8ecc00e341a98b6b607bccd52a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Image Embeddings Shape: (8640, 768)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 \n",
    "all_image_embeddings = []\n",
    "\n",
    "# Process images in batches\n",
    "for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "    # Load and preprocess a batch of images\n",
    "    batch_images = []\n",
    "    for path in image_paths[i:i + batch_size]:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        batch_images.append(image)\n",
    "\n",
    "    # Use the CLIP processor to prepare the batch of images\n",
    "    inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Obtain the image embeddings\n",
    "    with torch.no_grad():\n",
    "        image_embeddings = model.get_image_features(**inputs)\n",
    "\n",
    "    # Detach the embeddings and move to CPU\n",
    "    all_image_embeddings.append(image_embeddings.detach().cpu().numpy())\n",
    "\n",
    "# Convert the list of embeddings to a single NumPy array\n",
    "all_image_embeddings = np.concatenate(all_image_embeddings, axis=0)\n",
    "print(\"Total Image Embeddings Shape:\", all_image_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edeaceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_vis = {\"stimuli\": fmri_stim,\n",
    "            \"stimuli_paths\": image_paths,\n",
    "            \"embeddings\": all_image_embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9072e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file: /projects/crunchie/boyanova/EEG_Things/eeg_prep/scripts/CLIP_vis_fmri_512.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_data(CLIP_vis, \"/projects/crunchie/boyanova/EEG_Things/eeg_prep/scripts/CLIP_vis_fmri_512.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
