{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db861d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### libs\n",
    "import numpy as np\n",
    "import pickle \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ad91bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom funcs\n",
    "# Prepare hooks to capture activations\n",
    "activations_alexnet = {}\n",
    "def get_activation(name, activations_dict):\n",
    "    def hook(model, input, output):\n",
    "        activations_dict[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# load and preprocess images\n",
    "def load_image(image_path):\n",
    "     # ensure image is in RGB format\n",
    "    img = Image.open(image_path).convert('RGB') \n",
    "    img = preprocess(img)\n",
    "    return img.unsqueeze(0)  # add batch dimension\n",
    "\n",
    "# process and collect activations for each image\n",
    "def extract_activations(image_paths, model, activations_dict):\n",
    "    activs = {\"0\" : [],\n",
    "              \"1\" : []}\n",
    "    for image_path in image_paths:\n",
    "        image = load_image(image_path)\n",
    "        model(image)\n",
    "        for key in activs.keys():\n",
    "            activs[key].append(activations_dict[key].detach().numpy().flatten())\n",
    "    return activs\n",
    "\n",
    "def min_max_normalize(array):\n",
    "    min_vals = array.min(axis=0)\n",
    "    max_vals = array.max(axis=0)\n",
    "    normalized_array = (array - min_vals) / (max_vals - min_vals)\n",
    "    return normalized_array\n",
    "\n",
    "def z_score_normalize(array):\n",
    "    mean_vals = array.mean(axis=0)\n",
    "    std_vals = array.std(axis=0)\n",
    "    normalized_array = (array - mean_vals) / std_vals\n",
    "    return normalized_array\n",
    "\n",
    "def get_rdms(actD):\n",
    "    rdms = []  \n",
    "    for key in actD:\n",
    "        arr = np.array(actD[key])\n",
    "        norm_arr = z_score_normalize(arr) \n",
    "        rdms.append(np.corrcoef(arr))\n",
    "        \n",
    "    return rdms\n",
    "\n",
    "def upper(df):\n",
    "    try:\n",
    "        assert(type(df)==np.ndarray)\n",
    "    except:\n",
    "        if type(df)==pd.DataFrame:\n",
    "            df = df.values\n",
    "        else:\n",
    "            raise TypeError('Must be np.ndarray or pd.DataFrame')\n",
    "    mask = np.triu_indices(df.shape[0], k=1)\n",
    "    return df[mask]\n",
    "\n",
    "def dump_data(data, filename):\n",
    "    print('writing file: ' + filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85d64848",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load paths\n",
    "category = \"elephant\"\n",
    "sim_sim = np.load(\"{}_sim_sim.npy\".format(category))\n",
    "sim_sim_var = np.load(\"{}_sim_sim_var.npy\".format(category))\n",
    "dissim_dissim = np.load(\"{}_dissim_dissim.npy\".format(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3949a02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7ff3d95ba850>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load model and attach hooks\n",
    "\n",
    "# load the pretrained AlexNet model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "# register hooks for desired layers in AlexNet\n",
    "alexnet.features[3].register_forward_hook(get_activation('0', activations_alexnet))\n",
    "alexnet.classifier[4].register_forward_hook(get_activation('1', activations_alexnet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5db604c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-d35a28484781>:36: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_array = (array - mean_vals) / std_vals\n"
     ]
    }
   ],
   "source": [
    "### extract \n",
    "# define image preprocessing pipeline \n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "correlations = []\n",
    "data_dict = {\"paths\": [],\n",
    "             \"activations\": []}\n",
    "### example extraction loop\n",
    "for img_path in sim_sim:\n",
    "    data_dict[\"paths\"].append(img_path)\n",
    "    \n",
    "    # Get a flattened version of the activations\n",
    "    flattened_activations = extract_activations(img_path, alexnet, activations_alexnet)\n",
    "    data_dict[\"activations\"].append(flattened_activations)\n",
    "    \n",
    "    rdms = get_rdms(flattened_activations)\n",
    "    correlation, _ = stats.spearmanr(upper(rdms[0]), upper(rdms[1]))\n",
    "    correlations.append(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f35c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file: elephant_raw_activations_alexnet.pkl\n"
     ]
    }
   ],
   "source": [
    "### you can decide what format of the data you want to save\n",
    "dump_data(data_dict, \"{}_raw_activations_alexnet.pkl\".format(category))\n",
    "np.save(\"{}_spearmanns_alexnet.npy\".format(category), correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "699e1fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 3, 224, 224])\n",
      "Output of layer 0 (Conv2d): (193600,)\n",
      "Output of layer 1 (ReLU): (193600,)\n",
      "Output of layer 2 (MaxPool2d): (46656,)\n",
      "Output of layer 3 (Conv2d): (139968,)\n",
      "Output of layer 4 (ReLU): (139968,)\n",
      "Output of layer 5 (MaxPool2d): (32448,)\n",
      "Output of layer 6 (Conv2d): (64896,)\n",
      "Output of layer 7 (ReLU): (64896,)\n",
      "Output of layer 8 (Conv2d): (43264,)\n",
      "Output of layer 9 (ReLU): (43264,)\n",
      "Output of layer 10 (Conv2d): (43264,)\n",
      "Output of layer 11 (ReLU): (43264,)\n",
      "Output of layer 12 (MaxPool2d): (9216,)\n",
      "\n",
      "After flattening: torch.Size([1, 9216])\n",
      "Output of classifier layer 0 (Dropout): (9216,)\n",
      "Output of classifier layer 1 (Linear): (4096,)\n",
      "Output of classifier layer 2 (ReLU): (4096,)\n",
      "Output of classifier layer 3 (Dropout): (4096,)\n",
      "Output of classifier layer 4 (Linear): (4096,)\n",
      "Output of classifier layer 5 (ReLU): (4096,)\n",
      "Output of classifier layer 6 (Linear): (1000,)\n"
     ]
    }
   ],
   "source": [
    "### validate that sizes are correct based on the layers you've chosen \n",
    "input_tensor = torch.randn(1, 3, 224, 224) \n",
    "x = input_tensor\n",
    "print(\"Input:\", x.shape)\n",
    "\n",
    "for i, layer in enumerate(alexnet.features):\n",
    "    x = layer(x)\n",
    "    print(f\"Output of layer {i} ({layer.__class__.__name__}): {x.detach().numpy().flatten().shape}\")\n",
    "\n",
    "# passing through the classifier part\n",
    "# flatten the output of the conv layers to pass to the fully connected layers\n",
    "x = torch.flatten(x, 1)  \n",
    "print()\n",
    "print(f\"After flattening: {x.shape}\")\n",
    "\n",
    "for i, layer in enumerate(alexnet.classifier):\n",
    "    x = layer(x)\n",
    "    print(f\"Output of classifier layer {i} ({layer.__class__.__name__}): {x.detach().numpy().flatten().shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
