{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d9d552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838a6527b99745e3ab96a55e1c4aa94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/crunchie/boyanova/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b49c9688fd48caa30e09ca4ec6f972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3451925029b47c4afda3d3c0498f827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc11204d26443dc93ca01765fcea0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb9e7e2e14c42fda32f36d629078465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6452bb80c1c54121aa02646959f3482d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8496f06000ed4cd6b10f3034461e43b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af0448983254f74badc9cb78b70411b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626fe834e41947f4b72eb21494e5563f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def dump_data(data, filename):\n",
    "    print('writing file: ' + filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load CLIP model and processor\n",
    "project_dir = \"/projects/crunchie/boyanova/EEG_Things/Grouping-Embeddings\"\n",
    "image_dir = \"/projects/crunchie/boyanova/EEG_Things/data_set/Images\"\n",
    "\n",
    "cache_dir = os.path.join(project_dir, \"models\")\n",
    "fmri_stim = np.load(os.path.join(project_dir, \"files\", \"fmri_train_stim.npy\"), allow_pickle=True)\n",
    "imagePaths = []\n",
    "\n",
    "for im in tqdm(fmri_stim):\n",
    "    im_cat = im.split(\".\")[0][0:-4]\n",
    "    imagePaths.append(os.path.join(image_dir, im_cat, im))\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\",         \n",
    "                                  torch_dtype=torch.float16,\n",
    "                                  device_map=\"auto\",\n",
    "                                  cache_dir=cache_dir)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\",                                                                             cache_dir=cache_dir)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a11b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22095420b9e346478e592bdde2fe411e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Image Embeddings Shape: (8640, 768)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 \n",
    "all_image_embeddings = []\n",
    "\n",
    "# Process images in batches\n",
    "for i in tqdm(range(0, len(imagePaths), batch_size)):\n",
    "    # Load and preprocess a batch of images\n",
    "    batch_images = []\n",
    "    for path in imagePaths[i:i + batch_size]:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        batch_images.append(image)\n",
    "\n",
    "    # Use the CLIP processor to prepare the batch of images\n",
    "    inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "    # Obtain the image embeddings\n",
    "    with torch.no_grad():\n",
    "        image_embeddings = model.get_image_features(**inputs)\n",
    "\n",
    "    # Detach the embeddings and move to CPU\n",
    "    all_image_embeddings.append(image_embeddings.detach().cpu().numpy())\n",
    "\n",
    "# Convert the list of embeddings to a single NumPy array\n",
    "all_image_embeddings = np.concatenate(all_image_embeddings, axis=0)\n",
    "print(\"Total Image Embeddings Shape:\", all_image_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edeaceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_vis = {\"stimuli\": fmri_stim,\n",
    "            \"stimuli_paths\": imagePaths,\n",
    "            \"embeddings\": all_image_embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9072e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file: /projects/crunchie/boyanova/EEG_Things/eeg_prep/scripts/CLIP_vis_fmri_512.pickle\n"
     ]
    }
   ],
   "source": [
    "file_name = \"CLIP_vis_fmri.pickle\"\n",
    "save_dir = os.path.join(project_dir, \"files\", file_name)\n",
    "dump_data(CLIP_vis, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
