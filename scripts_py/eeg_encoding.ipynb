{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Explanation of the approach (Theory, reasoning)\n",
    "**Pre-trained CLIP Embeddings vs. Raw EEG Data**\n",
    "\n",
    "- CLIP embeddings: CLIP is a model that has been pre-trained on a massive dataset of images and text, so its embeddings contain rich semantic and visual information. These embeddings are already in a high-dimensional space that captures complex features of images and their associated textual descriptions.\n",
    "- EEG data: EEG signals, on the other hand, are raw brain activity measurements that are highly noisy and subject-specific. The signal varies depending on individual neurological patterns, making it difficult to directly extract meaningful high-level features from EEG.\n",
    "\n",
    "*Why EEG → CLIP works better:*\n",
    "\n",
    "- Pre-trained semantic knowledge: CLIP embeddings already provide a well-established representation of visual and textual information. By mapping EEG signals to these rich, pre-trained embeddings, you’re leveraging the model’s learned understanding of the world and associating brain activity with meaningful, high-level features (such as whether the brain is processing visual or semantic information). In contrast, if you tried to map CLIP embeddings to EEG, you’d be trying to generate EEG signals from high-level, abstract representations, which is much more difficult because EEG signals don’t directly encode the structured semantics or visual information that CLIP does.\n",
    "\n",
    "**Meaningful Associations (Visual vs. Semantic)**\n",
    "\n",
    "The goal is to identify *when* the EEG data corresponds to visual vs. semantic information from CLIP embeddings. EEG data reflects brain activity related to perception, memory, or cognition, which directly aligns with visual or semantic processing in the brain.\n",
    "*EEG → CLIP* allows us to train the model to understand which brain patterns correspond to which CLIP embeddings (visual or semantic), making it easier to match brain activity to its associated high-level features in CLIP.\n",
    "\n",
    "*Why EEG → CLIP is preferred:*\n",
    "\n",
    "The brain's neural responses are grounded in perception and cognition (which are visual or semantic in nature). This means that mapping EEG to CLIP allows you to uncover how different brain states (such as processing visual vs. semantic information) are reflected in the EEG data.\n",
    "\n",
    "**!!** CLIP → EEG would not have a clear mapping because CLIP is designed to produce high-level embeddings, not brain signals. The relationship between EEG data and CLIP embeddings is more about mapping the brain's interpretation of images or text, rather than generating EEG signals from abstract embeddings.\n",
    "\n",
    "**Implementation**\n",
    "Things which one needs to consider about X in a linear regression:\n",
    "- N x F (samples x features): N should always be more than F\n",
    "- I have 4 images x 150 reps x 64 channels x 900 time points \n",
    "- y needs to be 4 x 150 x 768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Simulate captions (LLAVA rich)\n",
    "### 2. Extract CLIP embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use a Simple Model\n",
    "Start with a linear regression or a simple multi-layer perceptron (MLP):\n",
    "- **Input:** Flattened EEG data (4 × 150 x 64 × 30 → 4*150=600 x 64) [x 90 (time points), / 2]; train 75% vs test 25% (LOIO approach)\n",
    "- **Output:** CLIP embeddings (4 × 768).\n",
    "\n",
    "A simple model is less likely to overfit your limited data and can still learn a basic mapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Code Skeleton idea (Ridge Regression, L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing on Image [0] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time points:   0%|          | 0/45 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time points: 100%|██████████| 45/45 [00:02<00:00, 15.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing on Image [1] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time points: 100%|██████████| 45/45 [00:02<00:00, 20.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing on Image [2] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time points: 100%|██████████| 45/45 [00:00<00:00, 62.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing on Image [3] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time points: 100%|██████████| 45/45 [00:02<00:00, 15.93it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# -----------------------------\n",
    "# Configurable Parameters\n",
    "# -----------------------------\n",
    "chunk_size = 2  # Number of time points to group together (set to 1 for no chunking)\n",
    "alpha = 1.0     # Regularization strength for Ridge regression\n",
    "\n",
    "# -----------------------------\n",
    "# Simulated Data (Replace with real EEG & CLIP embeddings)\n",
    "# -----------------------------\n",
    "num_images = 4\n",
    "num_trials = 150\n",
    "num_channels = 64\n",
    "num_timepoints = 90\n",
    "clip_dim = 768  # CLIP embedding size\n",
    "\n",
    "# EEG Data: (Images x Trials x Channels x Time)\n",
    "EEG_data = np.random.rand(num_images, num_trials, num_channels, num_timepoints)  \n",
    "# CLIP Embeddings: (Images x 768)\n",
    "CLIP_embeddings = np.random.rand(num_images, clip_dim)  \n",
    "\n",
    "# -----------------------------\n",
    "# Leave-One-Image-Out CV\n",
    "# -----------------------------\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for train_image_idxs, test_image_idx in loo.split(np.arange(num_images)):\n",
    "    print(f\"\\n--- Testing on Image {test_image_idx} ---\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Prepare Training Data\n",
    "    # -----------------------------\n",
    "    train_EEG = EEG_data[train_image_idxs]  # (3, 150, 64, 90)\n",
    "    train_CLIP = CLIP_embeddings[train_image_idxs]  # (3, 768)\n",
    "\n",
    "    # Reshape: Collapse images and trials into one dimension\n",
    "    train_EEG = train_EEG.reshape(-1, num_channels, num_timepoints)  # (3*150=450, 64, 90)\n",
    "\n",
    "    # Whiten data: Guggenmos approach (create a function)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Prepare Test Data\n",
    "    # -----------------------------\n",
    "    test_EEG = EEG_data[test_image_idx]  # (150, 64, 90)\n",
    "    test_CLIP = CLIP_embeddings[test_image_idx]  # (768,)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Train Separate Models for Each Time Point\n",
    "    # -----------------------------\n",
    "    for t in tqdm(range(0, num_timepoints - chunk_size + 1, chunk_size), desc = \"Time points\"):\n",
    "        # Chunk time points if needed\n",
    "        train_X = train_EEG[:, :, t:t+chunk_size].reshape(450, -1)  # (450, chunk_size * 64)\n",
    "        test_X = test_EEG[:, :, :, t:t+chunk_size].reshape(150, -1)  # (150, chunk_size * 64)\n",
    "\n",
    "        # Target (CLIP Embeddings)\n",
    "        train_y = np.repeat(train_CLIP, num_trials, axis=0)  # Expand to (450, 768)\n",
    "        test_y = np.tile(test_CLIP, (150, 1))  # Expand to (150, 768)\n",
    "\n",
    "        # Ridge Regression (Can switch to MLP later)\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(train_X, train_y)\n",
    "\n",
    "        # Predictions\n",
    "        test_preds = model.predict(test_X)\n",
    "\n",
    "        # Compute Cosine Similarity\n",
    "        cosine_similarity = np.mean(np.sum(test_preds * test_y, axis=1) / \n",
    "                                    (np.linalg.norm(test_preds, axis=1) * np.linalg.norm(test_y, axis=1)))\n",
    "\n",
    "        #print(f\"Time {t}-{t+chunk_size-1}: Cosine Similarity = {cosine_similarity:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150, 64, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_EEG[:, :, :, t:t+chunk_size].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 64, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_EEG[:, :, t:t+chunk_size].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
