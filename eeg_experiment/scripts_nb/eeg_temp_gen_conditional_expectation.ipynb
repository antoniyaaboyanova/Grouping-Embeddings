{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c9c5a6",
   "metadata": {},
   "source": [
    "### Do temp gen between det and rand (4) \n",
    "    ## - train on rand test on det (fix)\n",
    "    ## - train on det test on rand (fix)\n",
    "    ## - train on rand test on det (img)\n",
    "    ## - train on det test on rand (img)\n",
    "    \n",
    "    \n",
    "### What questions can we resolve with this type of analysis? \n",
    "\n",
    "Here we do the training on a partition of the data which is within the same task (fixation vs image), but within a specifi image presentation order (deterministic vs random). What we are trying to estimate is how well can a classifier trained on either deterministic or random image presentations pick out the stimulus-information from the corresponding order.  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7e2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from sklearn.discriminant_analysis import _cov\n",
    "import scipy\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def load_data(file):\n",
    "   \n",
    "    print('loading file: ' + file)\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "def dump_data(data, filename):\n",
    "    print('writing file: ' + filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def subsample_data(dat, sub_factor):\n",
    "    dshape = dat['eeg'].shape\n",
    "    sub_ix = list(range(0, dshape[-1], sub_factor))\n",
    "    dat['eeg'] = dat['eeg'][:,:,sub_ix]\n",
    "    dat['time'] = dat['time'][sub_ix]\n",
    "    \n",
    "    return dat\n",
    "    \n",
    "\n",
    "def get_pseudotrials(eeg_dat, tr_num):\n",
    "    \"\"\"\n",
    "    Applies pseudotrial creation independently for each slice of the first leading dimension.\n",
    "    \n",
    "    Parameters:\n",
    "        eeg_dat (numpy.ndarray): EEG data with shape (2, 4, 150, 64, 90).\n",
    "        tr_num (int): Desired number of trials.\n",
    "    \n",
    "    Returns:\n",
    "        pst (numpy.ndarray): Pseudotrials with shape (2, 4, tr_num, 64, 90).\n",
    "        k (int): Number of chunks each trial is divided into.\n",
    "    \"\"\"\n",
    "    # Prepare to store results\n",
    "    results = []\n",
    "    k_values = []\n",
    "\n",
    "    # Iterate over the first leading dimension\n",
    "    for i in range(eeg_dat.shape[0]):\n",
    "        single_data = eeg_dat[i]  # Extract slice with shape (4, 150, 64, 90)\n",
    "        \n",
    "        shape = single_data.shape\n",
    "        k = shape[1]  # Start with the number of trials\n",
    "        l = int(shape[1] / k)\n",
    "        \n",
    "        # Adjust k and l to fit tr_num\n",
    "        while l < int(tr_num):\n",
    "            k = k - 1\n",
    "            l = int(shape[1] / k)\n",
    "\n",
    "        # Shuffle and reshape for pseudotrials\n",
    "        single_data = single_data[:, np.random.permutation(shape[1]), :, :]\n",
    "        single_data = single_data[:, :l*k, :, :]\n",
    "        single_data = single_data.reshape(shape[0], k, l, shape[2], shape[3])\n",
    "        \n",
    "        # Average across the pseudotrial axis\n",
    "        pst_single = single_data.mean(axis=1)  # Resulting shape: (4, l, 64, 90)\n",
    "\n",
    "        results.append(pst_single)  # Append results for this slice\n",
    "        k_values.append(k)\n",
    "\n",
    "    # Stack results along the first axis to combine (2, 4, l, 64, 90)\n",
    "    pst = np.stack(results, axis=0)\n",
    "    k = k_values[0]  # Assuming k is the same across all slices\n",
    "    \n",
    "    return pst, k\n",
    "\n",
    "\n",
    "def average_across_points(dat, window_size=10):\n",
    "    dshape = dat['eeg'].shape\n",
    "    \n",
    "    new_length = dshape[-1] // window_size\n",
    "    eeg_reshaped = dat['eeg'][:, :, :new_length * window_size].reshape(dshape[0], dshape[1], new_length, window_size)\n",
    "    dat['eeg'] = eeg_reshaped.mean(axis=-1)\n",
    "    dat['time'] = dat['time'][:new_length * window_size].reshape(new_length, window_size).mean(axis=-1)\n",
    "    \n",
    "    return dat\n",
    "\n",
    "def select_partition(data, cond):\n",
    "    \n",
    "    ### Select condition \n",
    "    image_labels = [1,2,3,4]\n",
    "    if cond == \"rand\":\n",
    "        image_labels = [im + 10 for im in image_labels]\n",
    "\n",
    "    mask = np.isin(dat[\"ids\"], image_labels)\n",
    "\n",
    "    eeg = dat[\"eeg\"][mask]\n",
    "    ids = dat[\"ids\"][mask]\n",
    "    return eeg, ids\n",
    "\n",
    "def random_eeg_pick(eeg, ids):\n",
    "    eeg_svm = np.full((len((np.unique(ids))), trial_lim, eeg.shape[1], eeg.shape[2]), np.nan)\n",
    "    for idx, x in enumerate(np.unique(ids)):\n",
    "        \n",
    "        total_num_trials = len(ids[ids == x])\n",
    "        range_array = np.arange(0, total_num_trials)\n",
    "        random_numbers = np.random.choice(range_array, trial_lim, replace=False)\n",
    "        # Select\n",
    "        eeg_svm[idx, :, :, :] = eeg[ids == x][random_numbers, :, :]\n",
    "        \n",
    "    return eeg_svm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: fix_det, Testing on: fix_rand\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/eeg_epoched/eeg_things_0005_fix.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5076f8ecb21f4b72a0b0e8d6530a7931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n",
      "(2, 4, 150, 64, 90)\n",
      "(2, 4, 12, 64, 90)\n",
      "cv: 1, out of: 6\n",
      "cv: 2, out of: 6\n",
      "cv: 3, out of: 6\n",
      "cv: 4, out of: 6\n",
      "cv: 5, out of: 6\n",
      "cv: 6, out of: 6\n"
     ]
    }
   ],
   "source": [
    "### Load Data\n",
    "sub = 5\n",
    "conditions_1 = [\"fix\", \"img\"]\n",
    "conditions_2 = [\"det\", \"rand\"]\n",
    "subsample_factor = 10\n",
    "testsize = 0.2\n",
    "trial_num = 12\n",
    "img_nperms = 25\n",
    "trial_lim = 150\n",
    "\n",
    "decoding_data = {}\n",
    "\n",
    "pairs = [(\"fix_det\", \"fix_rand\"),\n",
    "         (\"fix_rand\", \"fix_det\"),\n",
    "         (\"img_det\", \"img_rand\"),\n",
    "         (\"img_rand\", \"img_det\")]\n",
    "\n",
    "for pair_train, pair_test in pairs:\n",
    "    print(f\"Training on: {pair_train}, Testing on: {pair_test}\")\n",
    "    cond_name = f\"{pair_train}_{pair_test}\"\n",
    "    train_out = pair_train.split(\"_\")[0]\n",
    "    train_in = pair_train.split(\"_\")[1]\n",
    "    \n",
    "    test_out = pair_test.split(\"_\")[0]\n",
    "    test_in = pair_test.split(\"_\")[1]\n",
    "    \n",
    "    if train_out == test_out:\n",
    "        dat_name = f\"/projects/crunchie/boyanova/EEG_Things/eeg_experiment/eeg_epoched/eeg_things_{sub:04d}_{train_out}.pickle\"\n",
    "        dat = load_data(dat_name)\n",
    "        \n",
    "        ### Subsample data\n",
    "        dat = average_across_points(dat, window_size=10)\n",
    "\n",
    "        ### Button press mask\n",
    "        bt_press = dat[\"button_press_mask\"]\n",
    "        dat[\"eeg\"] = dat[\"eeg\"][~bt_press]\n",
    "        dat[\"ids\"] = dat[\"ids\"][~bt_press]\n",
    "        dat[\"block_num\"] = dat[\"block_num\"][~bt_press]\n",
    "        \n",
    "        ### Get vars\n",
    "        n_conditions = len(range(4))\n",
    "        n_sensors = dat[\"eeg\"].shape[1]\n",
    "        n_time = dat[\"eeg\"].shape[-1]\n",
    "\n",
    "        ### DA matrix \n",
    "        TG = np.full((n_conditions, n_conditions, n_time, n_time), np.nan)        \n",
    "        train_eeg, train_ids = select_partition(dat, train_in)\n",
    "        test_eeg, test_ids = select_partition(dat, test_in)\n",
    "          \n",
    "        for p in tqdm(range(img_nperms)):\n",
    "            eeg_svm_train = random_eeg_pick(train_eeg, train_ids)\n",
    "            eeg_svm_test = random_eeg_pick(test_eeg, test_ids)\n",
    "\n",
    "            eeg_general = np.stack((eeg_svm_train, eeg_svm_test), axis=0)\n",
    "            print(eeg_general.shape)\n",
    "\n",
    "            pstrials, binsize = get_pseudotrials(eeg_general, trial_num)\n",
    "            print(pstrials.shape)\n",
    "            n_pstrials = pstrials.shape[2]\n",
    "            n_test = int(n_pstrials * testsize)\n",
    "            ps_ixs = np.arange(n_pstrials)\n",
    "            cvs = int(n_pstrials / n_test)\n",
    "\n",
    "            for cv in range(cvs):\n",
    "                print('cv: {}, out of: {}'.format(cv+1, cvs))\n",
    "\n",
    "                # we take idxs for the test/train\n",
    "                test_ix = np.arange(n_test) + (cv * n_test)\n",
    "                train_ix = np.delete(ps_ixs.copy(), test_ix)\n",
    "\n",
    "                # subset idxs from the pseudotrials\n",
    "                ps_train = pstrials[0]\n",
    "                ps_test = pstrials[1]\n",
    "                ps_train = ps_train[:,train_ix,:,:]\n",
    "                ps_test = ps_test[:,test_ix,:,:]\n",
    "\n",
    "                sigma_ = np.empty((n_conditions, n_sensors, n_sensors))\n",
    "                for c in range(n_conditions):\n",
    "                    # compute sigma for each time point, then average across time\n",
    "                    sigma_[c] = np.mean([_cov(ps_train[c, :, :, t], shrinkage='auto')\n",
    "                                        for t in range(n_time)], axis=0)\n",
    "                \n",
    "                # average across conditions\n",
    "                sigma = sigma_.mean(axis=0)  \n",
    "                # the formula is sigma * -1/2 // reason for sigma_inv\n",
    "                sigma_inv = scipy.linalg.fractional_matrix_power(sigma, -0.5)\n",
    "\n",
    "                # apply sigma to pseudo trials \n",
    "                ps_train = (ps_train.swapaxes(2, 3) @ sigma_inv).swapaxes(2, 3)\n",
    "                ps_test = (ps_test.swapaxes(2, 3) @ sigma_inv).swapaxes(2, 3)\n",
    "\n",
    "                # decoding: cA image vs cB (cA + 1 :) // then do it for each time point \n",
    "                for cA in range(n_conditions):\n",
    "                    #print('decoding image ' + str(cA))\n",
    "                    for cB in range(cA+1, n_conditions):\n",
    "                        for t in range(n_time):\n",
    "                            # retrieve the patterns from pseudotrials that correspond to cA and cB at time pt t\n",
    "                            train_x = np.array((ps_train[cA,:,:,t], ps_train[cB,:,:,t]))\n",
    "                            # concatinate them\n",
    "                            train_x = np.reshape(train_x,(len(train_ix)*2, n_sensors))\n",
    "                            # do the same with the test set, but here we take all time points \n",
    "                            test_x = np.array((ps_test[cA], ps_test[cB]))\n",
    "                            test_x = np.reshape(test_x,(len(test_ix)*2, n_sensors, n_time))\n",
    "                            # config labesls 1 for cA and 2 for cB\n",
    "                            train_y = np.array([1] * len(train_ix) + [2] * len(train_ix))\n",
    "                            test_y = np.array([1] * len(test_ix) + [2] * len(test_ix))\n",
    "\n",
    "                            # instantiate a classifier \n",
    "                            classifier = LinearSVC(dual=True,\n",
    "                                                    penalty = 'l2',\n",
    "                                                    loss = 'hinge',\n",
    "                                                    C = .5,\n",
    "                                                    multi_class = 'ovr',\n",
    "                                                    fit_intercept = True,\n",
    "                                                    max_iter = 10000)\n",
    "                            # train it\n",
    "                            classifier.fit(train_x, train_y)\n",
    "                            for tt in range(n_time):\n",
    "                                pred_y = classifier.predict(test_x[:,:,tt])\n",
    "                                acc_score = accuracy_score(test_y,pred_y)\n",
    "                                # we store the acc score in the temp gen mattrix \n",
    "                                TG[cA,cB,t,tt] = np.nansum(np.array((TG[cA,cB,t,tt],acc_score)))\n",
    "        TG = TG / (img_nperms * cvs)\n",
    "        decoding_data[cond_name] = TG\n",
    "        \n",
    "dump_data(decoding_data, \"/projects/crunchie/boyanova/EEG_Things/eeg_experiment/eeg_decoding/eeg_TG_between_expectation_{:04d}.pickle\".format(sub))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
