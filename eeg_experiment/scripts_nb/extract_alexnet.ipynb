{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39482807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/crunchie/boyanova/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/projects/crunchie/boyanova/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "writing file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "writing file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "writing file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "Feature extraction completed.\n",
      "conv1\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "ReLU1\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "maxpool1\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "conv2\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "ReLU2\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "maxpool2\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "fc7\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "ReLU7\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "fc8\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/tomato_11s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/chandelier_10s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/blazer_08s.pickle\n",
      "loading file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/dustpan_01b.pickle\n",
      "writing file: /projects/crunchie/boyanova/EEG_Things/eeg_experiment/model_activations/alexnet/standardized_maps.pickle\n",
      "Feature group standarization complete!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "from torchvision import transforms as trn\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def dump_data(data, filename):\n",
    "    print('writing file: ' + filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_data(file):\n",
    "   \n",
    "    print('loading file: ' + file)\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "# Set paths and options\n",
    "PRETRAINED = True  # Use pretrained AlexNet model\n",
    "PROJECT_DIR = \"/projects/crunchie/boyanova/EEG_Things/\"\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 20200220\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# =============================================================================\n",
    "# Define AlexNet model class with layer extraction\n",
    "# =============================================================================\n",
    "# Define convolutional and fully connected layers of interest\n",
    "conv_layers = ['conv1', 'ReLU1', 'maxpool1', 'conv2', 'ReLU2', 'maxpool2',\n",
    "               'conv3', 'ReLU3', 'conv4', 'ReLU4', 'conv5', 'ReLU5', 'maxpool5']\n",
    "fully_connected_layers = ['Dropout6', 'fc6', 'ReLU6', 'Dropout7', 'fc7', 'ReLU7', 'fc8']\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"Select the desired layers and create the model.\"\"\"\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.select_cov = ['conv1', 'ReLU1', 'maxpool1', 'conv2', 'ReLU2', 'maxpool2']\n",
    "        self.select_fully_connected = ['fc7', 'ReLU7', 'fc8']\n",
    "        self.feat_list = self.select_cov + self.select_fully_connected\n",
    "        self.alex_feats = models.alexnet(pretrained=pretrained).features\n",
    "        self.alex_classifier = models.alexnet(pretrained=pretrained).classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract the feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.alex_feats._modules.items():\n",
    "            x = layer(x)\n",
    "            if conv_layers[int(name)] in self.feat_list:\n",
    "                features.append(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for name, layer in self.alex_classifier._modules.items():\n",
    "            x = layer(x)\n",
    "            if fully_connected_layers[int(name)] in self.feat_list:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "# Instantiate the model\n",
    "model = AlexNet(pretrained=PRETRAINED)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# =============================================================================\n",
    "# Define image preprocessing\n",
    "# =============================================================================\n",
    "centre_crop = trn.Compose([\n",
    "    trn.Resize((224, 224)),\n",
    "    trn.ToTensor(),\n",
    "    trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Process images and extract feature maps\n",
    "# =============================================================================\n",
    "# Directory to save extracted feature maps\n",
    "save_dir = os.path.join(PROJECT_DIR, 'eeg_experiment','model_activations', 'alexnet')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Get image list from specified directory\n",
    "\n",
    "experimental_stimuli = pd.read_csv(os.path.join(PROJECT_DIR, \"eeg_prep/scripts/exp_stimuli_desc.csv\"))\n",
    "experimental_stimuli = experimental_stimuli[\"stim_name\"].values\n",
    "image_list = [os.path.join(PROJECT_DIR, \"eeg_prep\", \"stimuli_shined\", x) for x in experimental_stimuli]\n",
    "\n",
    "# Extract and save feature maps\n",
    "for i, image_path in enumerate(image_list):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    input_img = V(centre_crop(img).unsqueeze(0))\n",
    "    if torch.cuda.is_available():\n",
    "        input_img = input_img.cuda()\n",
    "    \n",
    "    # Forward pass to extract features\n",
    "    feature_maps = model.forward(input_img)\n",
    "    feats = {model.feat_list[f]: feature_map.data.cpu().numpy() for f, feature_map in enumerate(feature_maps)}\n",
    "    \n",
    "    # Save feature maps\n",
    "    file_name = experimental_stimuli[i].split(\".\")[0] + \".pickle\"\n",
    "    dump_data(feats, os.path.join(save_dir, file_name))\n",
    "\n",
    "print(\"Feature extraction completed.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Standardize feature maps\n",
    "# =============================================================================\n",
    "\n",
    "folder_path = save_dir\n",
    "stimulus_feature_dict = {}\n",
    "\n",
    "\n",
    "standardized_maps = dict()\n",
    "for key in feats.keys():\n",
    "    print(key)\n",
    "    all_feature_maps = []\n",
    "    stimulus_names = []\n",
    "\n",
    "     \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pickle\"):  # Assuming feature maps are saved in pickle files\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            feature_map_dict = load_data(file_path)\n",
    "\n",
    "            # Extract the stimulus name (assuming it's the first key or a known key)\n",
    "            # Adjust 'stimulus_name' if the key is different\n",
    "            stimulus_name = feature_map_dict.get('stimulus_name') or list(feature_map_dict.keys())[0]\n",
    "            stimulus_names.append(filename.split(\".\")[0])\n",
    "\n",
    "            # Convert the rest of the feature map data to an array (exclude the name if needed)\n",
    "            feature_map_array = feature_map_dict[key].flatten()# Skip name if it's in first entry\n",
    "            all_feature_maps.append(feature_map_array)\n",
    "            concatenated_features = np.array(all_feature_maps)\n",
    "\n",
    "    # Apply StandardScaler to standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(concatenated_features)\n",
    "    standardized_maps[key] = concatenated_features\n",
    "    del scaled_features\n",
    "\n",
    "standardized_maps[\"stimulus_names\"] = stimulus_names \n",
    "dump_data(standardized_maps, os.path.join(save_dir, \"standardized_maps.pickle\"))\n",
    "print(\"Feature group standarization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5f8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
