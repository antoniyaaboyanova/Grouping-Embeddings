{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb730139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import pickle\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_data(file):\n",
    "\n",
    "    print('loading file: ' + file)\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "def dump_data(data, filename):\n",
    "    print('writing file: ' + filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def upper(df):\n",
    "\n",
    "    try:\n",
    "        assert(type(df)==np.ndarray)\n",
    "    except:\n",
    "        if type(df)==pd.DataFrame:\n",
    "            df = df.values\n",
    "        else:\n",
    "            raise TypeError('Must be np.ndarray or pd.DataFrame')\n",
    "    mask = np.triu_indices(df.shape[0], k=1)\n",
    "    return df[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8425e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rc('image', cmap='viridis')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",  \n",
    "    # use serif/main font for text elements\n",
    "    })\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams.update({'font.weight': 'bold'})\n",
    "plt.rcParams.update({'axes.linewidth': 2.5})\n",
    "plt.rcParams.update({'axes.labelweight': 'bold'})\n",
    "plt.rcParams.update({'axes.labelsize': 20})\n",
    "plt.rc('legend',fontsize=12)\n",
    "\n",
    "def plot_images(paths):\n",
    "    num_images = len(paths)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 5, 5))\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        im_name = path.split(\"/\")[-1].split(\".\")[0]\n",
    "        img = mpimg.imread(path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(im_name, fontsize = 16)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_images_to_pdf(paths, pdf_filename='images.pdf'):\n",
    "    with PdfPages(pdf_filename) as pdf:\n",
    "        for row in paths:\n",
    "            fig, axes = plt.subplots(1, len(row), figsize=(len(row) * 5, 5))\n",
    "            for j, path in enumerate(row):\n",
    "                img = mpimg.imread(path)\n",
    "                axes[j].imshow(img)\n",
    "                axes[j].axis('off')\n",
    "            pdf.savefig(fig) \n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e29e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/projects/crunchie/boyanova/EEG_Things/Grouping-Embeddings\"\n",
    "class_type = \"animate\"\n",
    "top_file = load_data(os.path.join(project_dir, \"files\", class_type, \"top25_CLIP_vis_blip.pickle\"))\n",
    "top_image_embeddings = top_file[\"image_embeddings\"]\n",
    "top_txt_embeddings = top_file[\"text_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffde87",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = top_file['cluster']\n",
    "cluster_categories = dict()\n",
    "for cl in np.unique(clusters):\n",
    "    indices = np.where(top_file[\"cluster\"] == cl) \n",
    "    concept = top_file[\"bigger_concept\"][indices]\n",
    "    \n",
    "    # Count occurrences of each word\n",
    "    concept = [x for x in concept if x != \"None\"]\n",
    "    word_counts = Counter(concept)\n",
    "\n",
    "    # Find the most frequent word\n",
    "    most_common_word, count = word_counts.most_common(1)[0]\n",
    "    print(f\"Cluster {cl}: {most_common_word}/{count}\")\n",
    "    cluster_categories[cl]=word_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20982389",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_struct = {}\n",
    "vis = top_image_embeddings\n",
    "vis_cluster = top_file[\"clusters\"]\n",
    "\n",
    "txt = top_txt_embeddings\n",
    "txt_cluster = top_file[\"clusters\"]\n",
    "stim_paths = top_file[\"stimuli_paths\"]\n",
    "stim = [x.split(\"/\")[-1] for x in stim_paths]\n",
    "\n",
    "for q in tqdm(range(len(vis))):\n",
    "    \n",
    "    # Step 1: Get Query \n",
    "    data_struct[stim[q]] = {}\n",
    "    \n",
    "    vis_embedding_q = vis[q]\n",
    "    vis_cluster_q = vis_cluster[q]\n",
    "    txt_embedding_q = txt[q]\n",
    "    txt_cluster_q = txt_cluster[q]\n",
    "\n",
    "    # Step 2: Keep all images from same cluserm but different category\n",
    "    overlap_indexes = np.where(vis_cluster == vis_cluster_q)[0]\n",
    "    overlap_indexes = np.insert(overlap_indexes, 0, q, axis=0)\n",
    "\n",
    "    # Step 3: Compute correlation RDMs\n",
    "    rdm_vis = 1 - np.corrcoef(vis[overlap_indexes])\n",
    "    rdm_txt = 1 - np.corrcoef(txt[overlap_indexes])\n",
    "    \n",
    "    # make diag high\n",
    "    np.fill_diagonal(rdm_vis, 1000)\n",
    "    np.fill_diagonal(rdm_txt, 1000) \n",
    "\n",
    "    #Step 4: Select first two\n",
    "    #selected = [0]\n",
    "    selected = []\n",
    "    vec1_vis = rdm_vis[0, ::]\n",
    "    vec1_txt = rdm_txt[0, ::]\n",
    "    mask = np.isin(np.arange(0, len(vec1_vis)), selected)\n",
    "    vec1_vis[mask] = 1000\n",
    "    vec1_txt[mask] = 1000\n",
    "    joint = vec1_vis + vec1_txt\n",
    "\n",
    "    min_ = np.argmin(joint)\n",
    "    selected.append(min_)\n",
    "\n",
    "    # update\n",
    "    for _ in range(3):\n",
    "        vec1_vis += rdm_vis[min_, ::]\n",
    "        vec1_txt += rdm_txt[min_, ::]\n",
    "        mask = np.isin(np.arange(0, len(vec1_vis)), selected)\n",
    "        vec1_vis[mask] = 1000\n",
    "        vec1_txt[mask] = 1000\n",
    "        joint = vec1_vis + vec1_txt\n",
    "        min_ = np.argmin(joint)\n",
    "        selected.append(min_)\n",
    "\n",
    "    data_struct[stim[q]][\"stimuli_paths\"] = stim_paths[overlap_indexes[selected]]\n",
    "    data_struct[stim[q]][\"indexes_masked\"] = overlap_indexes[selected]\n",
    "\n",
    "#dump_data(data_struct, \"selections_txt.pickle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d831b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "rhos = []\n",
    "for key in tqdm(data_struct.keys()):\n",
    "    indexes = data_struct[key][\"indexes_masked\"]\n",
    "    rdm_vis = 1 - np.corrcoef(vis[indexes])\n",
    "    rdm_txt = 1 - np.corrcoef(txt[indexes])\n",
    "    res = stats.spearmanr(upper(rdm_vis), upper(rdm_txt))\n",
    "    rhos.append(res.statistic)\n",
    "    \n",
    "    \n",
    "sorted_indexes = np.argsort(np.abs(rhos))    \n",
    "rhos = np.array(rhos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = np.array(list(data_struct.keys()))[np.where(np.abs(rhos) >= 0.2)]\n",
    "for key in sorted_keys[0:10]:\n",
    "    plot_images(data_struct[key][\"stimuli_paths\"])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = np.array(list(data_struct.keys()))[np.where(np.abs(rhos) > 0.9)]\n",
    "key = sorted_keys[1]\n",
    "indexes = data_struct[key][\"indexes_masked\"]\n",
    "# Calculate the RDMs for vision and text embeddings\n",
    "rdm_vis = 1 - np.corrcoef(vis[indexes])\n",
    "rdm_txt = 1 - np.corrcoef(txt[indexes])\n",
    "\n",
    "# Calculate Spearman correlation coefficient\n",
    "res = stats.spearmanr(upper(rdm_vis), upper(rdm_txt))\n",
    "corr = np.corrcoef(upper(rdm_vis), upper(rdm_txt))[0,1]\n",
    "\n",
    "\n",
    "# Plotting RDMs side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "# Plotting CLIP text RDM\n",
    "im1 = axes[0].imshow(rdm_txt, vmin=0.0, vmax=1.0, cmap=\"viridis\")\n",
    "axes[0].set_title(\"CLIP Text RDM\", fontsize = 12)\n",
    "fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Plotting CLIP vision RDM\n",
    "im2 = axes[1].imshow(rdm_vis, vmin=0.0, vmax=1.0, cmap=\"viridis\")\n",
    "axes[1].set_title(\"CLIP Vision RDM\", fontsize = 12)\n",
    "fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Plotting scatter\n",
    "axes[2].scatter(upper(rdm_txt), upper(rdm_vis))\n",
    "axes[2].grid(True)\n",
    "axes[2].set_ylabel(\"CLIP Vis Corr\", fontsize = 12)\n",
    "axes[2].set_xlabel(\"CLIP Txt Corr\", fontsize = 12)\n",
    "\n",
    "# Display Spearman correlation coefficient in the subtitle\n",
    "fig.suptitle(f\"Spearman Correlation (RDMs): {res.statistic:.2f}\\n Pearson's Correlation (RDMs): {corr:.2f}\", fontsize=24)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "save_path = \"/projects/crunchie/boyanova/EEG_Things/eeg_prep/figures/00_rdm_desc.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the stimuli images\n",
    "plot_images(data_struct[key][\"stimuli_paths\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
